#!/usr/bin/env python3
"""
é€šç”¨æ¡æ¬¾åŒ¹é…è„šæœ¬
åŠŸèƒ½ï¼šè¯­ä¹‰å‘é‡å¬å› + LLMç²¾åˆ¤ + Excelå¯¼å‡º
é€‚ç”¨äºä»»æ„ä¸¤ä¸ªè´£ä»»æ ‡å‡†æ–‡æ¡£çš„æ¯”å¯¹
"""

import json
import os
import time
from typing import List, Dict, Tuple, Any
import requests
import numpy as np
import pandas as pd
import openai
from tqdm import tqdm

import faiss
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

# é…ç½® CUDA å†…å­˜ç®¡ç†ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'


# ==================== é…ç½®å‚æ•° ====================
class Config:
    # æ–‡ä»¶è·¯å¾„
    A_FILE = "/home/pmw/h20/Text_matching/RBA_A.json"
    B_FILE = "/home/pmw/h20/Text_matching/Apple_standard.json"
    OUTPUT_EXCEL = "/home/pmw/h20/Text_matching/General_matching_results.xlsx"
    OUTPUT_HTML = "/home/pmw/h20/Text_matching/General_matching_results.html"

    # Qwen3-Embedding-8B åµŒå…¥æ¨¡å‹ (Qwen/Qwen3-Embedding-8B)
    # æ”¯æŒæœ¬åœ°è·¯å¾„æˆ– Hugging Face æ¨¡å‹å
    EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-8B"
    # 8B æ¨¡å‹çº¦ 16GBï¼Œä½¿ç”¨ CPU è¿è¡Œï¼ˆç¨³å®šä½†è¾ƒæ…¢ï¼‰
    EMBEDDING_DEVICE = "cpu"

    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
    HF_CACHE = os.path.expanduser("~/.cache/huggingface/hub/models--Qwen--Qwen3-Embedding-8B")
    USE_LOCAL_ONLY = os.path.exists(HF_CACHE)  # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å¼

    # BGE-Reranker-Base æ¨¡å‹
    RERANKER_MODEL = "BAAI/bge-reranker-base"  # BGE Reranker æ¨¡å‹
    RERANKER_DEVICE = "cpu"  # Reranker ä½¿ç”¨ CPUï¼ŒèŠ‚çœæ˜¾å­˜
    RERANKER_TOP_K = 5  # Rerank åå– Top-K
    ENABLE_RERANKER = True  # Reranker å¼€å…³ï¼šTrue=å¯ç”¨ï¼ŒFalse=ç¦ç”¨

    # å‘é‡æ£€ç´¢å‚æ•°
    TOP_K = 20  # å¬å›Top-Kå€™é€‰ï¼ˆå¢å¤§å¬å›æ•°é‡ï¼‰

    # LLM API é…ç½®
    LLM_API_BASE = "http://10.71.5.24:8000/v1"
    LLM_API_KEY = "empty"  # æœ¬åœ°æœåŠ¡é€šå¸¸ä¸éœ€è¦key
    LLM_MODEL = "gpt-3.5-turbo"
    LLM_TIMEOUT = 60
    LLM_MAX_RETRIES = 3

    # ç›¸ä¼¼åº¦é˜ˆå€¼
    SIMILARITY_THRESHOLD = 0.8  # ä½äºæ­¤åˆ†æ•°çš„åŒ¹é…ä¸è¿›è¡ŒLLMåˆ¤æ–­


# ==================== å·¥å…·å‡½æ•° ====================
def load_json_documents(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONæ ¼å¼çš„æ–‡æ¡£ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. JSONæ•°ç»„æ ¼å¼ï¼š[{...}, {...}, ...]
    2. æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š{...}\n{...}\n...
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read().strip()

    # å°è¯•æ–¹å¼1: JSONæ•°ç»„æ ¼å¼
    try:
        documents = json.loads(content)
        if isinstance(documents, list):
            print(f"  æ£€æµ‹åˆ°JSONæ•°ç»„æ ¼å¼ï¼ŒåŠ è½½ {len(documents)} æ¡è®°å½•")
            return documents
    except json.JSONDecodeError:
        pass

    # å°è¯•æ–¹å¼2: æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
    documents = []
    lines = content.split('\n')
    line_num = 0
    while line_num < len(lines):
        line = lines[line_num].strip()
        if not line:
            line_num += 1
            continue

        try:
            # å°è¯•å•è¡Œè§£æ
            doc = json.loads(line)
            documents.append(doc)
            line_num += 1
        except json.JSONDecodeError:
            # å°è¯•å¤šè¡Œè§£æï¼ˆæ‰¾åˆ°å®Œæ•´çš„JSONå¯¹è±¡ï¼‰
            json_str = line
            nested_line = line_num + 1
            while nested_line < len(lines):
                next_line = lines[nested_line]
                json_str += '\n' + next_line
                try:
                    doc = json.loads(json_str)
                    documents.append(doc)
                    line_num = nested_line + 1
                    break
                except json.JSONDecodeError:
                    nested_line += 1
            else:
                print(f"è­¦å‘Š: ç¬¬{line_num + 1}è¡ŒJSONè§£æå¤±è´¥ï¼ˆå°è¯•äº†å¤šè¡Œåˆå¹¶ï¼‰")
                line_num += 1

    print(f"  æ£€æµ‹åˆ°æ¯è¡ŒJSONå¯¹è±¡æ ¼å¼ï¼ŒåŠ è½½ {len(documents)} æ¡è®°å½•")
    return documents


def filter_content_blocks(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """è¿‡æ»¤å‡ºåªåŒ…å« content å­—æ®µçš„æ–‡æ¡£å—ï¼ˆæ’é™¤ Preambleï¼‰"""
    content_docs = []
    for doc in documents:
        # åªä¿ç•™åŒ…å« 'content' å­—æ®µçš„å—
        if 'content' in doc:
            content_docs.append(doc)
    return content_docs


def build_embedding_text(doc: Dict[str, Any]) -> str:
    """æ„å»ºç”¨äº embedding çš„æ–‡æœ¬ï¼ŒåŒ…å«å±‚çº§è·¯å¾„ä¿¡æ¯

    æ ¼å¼: æ¯ä¸ªå±‚çº§ç‹¬å ä¸€è¡Œï¼Œæœ€åæ˜¯å†…å®¹
    ä¾‹å¦‚:
    Anti-Discrimination
    Supplier Code of Conduct
    Supplier Responsibility Standards
    1. Policy
    å…·ä½“å†…å®¹...
    """
    parts = []

    # æ·»åŠ å±‚çº§ä¿¡æ¯ï¼ˆæ¯ä¸ªå±‚çº§å•ç‹¬ä¸€è¡Œï¼‰
    if doc.get('Theme'):
        parts.append(doc['Theme'])

    level_1 = doc.get('level_1', {})
    if level_1 and level_1.get('title'):
        title = level_1['title']
        if level_1.get('id'):
            parts.append(f"{level_1['id']}. {title}")
        else:
            parts.append(title)

    level_2 = doc.get('level_2', {})
    if level_2 and level_2.get('title'):
        title = level_2['title']
        if level_2.get('id'):
            parts.append(f"{level_2['id']}. {title}")
        else:
            parts.append(title)

    level_3 = doc.get('level_3', {})
    if level_3 and level_3.get('title'):
        title = level_3['title']
        if level_3.get('id'):
            parts.append(f"{level_3['id']}. {title}")
        else:
            parts.append(title)

    # æ·»åŠ å®é™…å†…å®¹
    content = doc.get('content', '')
    if content:
        parts.append(content)

    return '\n'.join(parts)


def truncate_text(text: str, max_length: int = 512) -> str:
    """æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬"""
    if len(text) <= max_length:
        return text
    return text[:max_length] + "..."


# ==================== Qwen å‘é‡åµŒå…¥ ====================
class QwenEmbedder:
    """ä½¿ç”¨ Qwen3-Embedding-8B æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡

    Qwen Embedding æ¨¡å‹ä¸éœ€è¦å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬
    """

    def __init__(self, model_name: str = Config.EMBEDDING_MODEL, device: str = Config.EMBEDDING_DEVICE):
        print(f"æ­£åœ¨åŠ è½½ Qwen3-Embedding-8B æ¨¡å‹ ({device}æ¨¡å¼)...")
        self.device = device

        # ç¦ç”¨ huggingface_hub çš„ç½‘ç»œæ£€æŸ¥
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

        # è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„
        if Config.USE_LOCAL_ONLY:
            # æŸ¥æ‰¾å®é™…çš„ snapshot è·¯å¾„
            import glob
            snapshot_pattern = os.path.expanduser("~/.cache/huggingface/hub/models--Qwen--Qwen3-Embedding-8B/snapshots/*")
            snapshot_dirs = glob.glob(snapshot_pattern)
            if snapshot_dirs:
                # æ‰¾åˆ°åŒ…å«å®Œæ•´æ¨¡å‹æ–‡ä»¶çš„ snapshot
                valid_snapshot = None
                for snapshot_dir in snapshot_dirs:
                    if os.path.exists(os.path.join(snapshot_dir, "config.json")) and \
                       os.path.exists(os.path.join(snapshot_dir, "tokenizer.json")):
                        valid_snapshot = snapshot_dir
                        break

                if valid_snapshot:
                    print(f"  ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {valid_snapshot}")
                    model_to_load = valid_snapshot
                    use_local = True
                else:
                    print(f"  æœ¬åœ°ç¼“å­˜ä¸å®Œæ•´ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½: {model_name}")
                    model_to_load = model_name
                    use_local = False
            else:
                print(f"  æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½: {model_name}")
                model_to_load = model_name
                use_local = False
        else:
            print(f"  ä½¿ç”¨è¿œç¨‹æ¨¡å‹: {model_name}")
            model_to_load = model_name
            use_local = False

        load_kwargs = {
            "local_files_only": use_local,
            "trust_remote_code": True
        }

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_to_load, **load_kwargs)
            self.model = AutoModel.from_pretrained(model_to_load, **load_kwargs)
            self.model.to(device)
            self.model.eval()
            print("Qwen3-Embedding-8B æ¨¡å‹åŠ è½½å®Œæˆ")

        except Exception as e:
            print(f"\né”™è¯¯: æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("1. æ¸…ç†æ˜¾å­˜: python -c \"import torch; torch.cuda.empty_cache()\"")
            print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("3. è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒç«™: export HF_ENDPOINT=https://hf-mirror.com")
            print("4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹: huggingface-cli download Qwen/Qwen3-Embedding-8B")
            raise

    def encode_queries(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """å°†æŸ¥è¯¢æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        return self._encode(texts, batch_size)

    def encode_passages(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """å°†æ–‡æ¡£æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        return self._encode(texts, batch_size)

    def encode(self, texts: List[str], batch_size: int = 32, is_query: bool = False) -> np.ndarray:
        """å°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºå‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            is_query: Qwen Embedding ä¸åŒºåˆ† query/passageï¼Œæ­¤å‚æ•°ä¿ç•™å…¼å®¹æ€§
        """
        return self._encode(texts, batch_size)

    def _encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """å†…éƒ¨ç¼–ç æ–¹æ³•"""
        all_embeddings = []
        total_texts = len(texts)

        for i in range(0, total_texts, batch_size):
            batch_texts = texts[i:i + batch_size]
            current_batch_num = i // batch_size + 1
            total_batches = (total_texts + batch_size - 1) // batch_size

            print(f"  æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {current_batch_num}/{total_batches} ({len(batch_texts)} æ¡æ–‡æœ¬)...")

            # Tokenize
            encoded_input = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )

            # Move to device
            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

            # Encode
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                # Qwen ä½¿ç”¨å¹³å‡æ± åŒ–
                embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
                # å½’ä¸€åŒ–
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

            # ç«‹å³ç§»åˆ° CPU å¹¶é‡Šæ”¾ GPU æ˜¾å­˜
            all_embeddings.append(embeddings.cpu().numpy())

            # æ¸…ç†ä¸­é—´å˜é‡
            del embeddings, encoded_input, model_output
            if self.device == "cuda":
                torch.cuda.empty_cache()

        return np.vstack(all_embeddings)

    def _mean_pooling(self, model_output, attention_mask):
        """å¹³å‡æ± åŒ–"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# ==================== å‘é‡ç´¢å¼• ====================
class VectorIndex:
    """ä½¿ç”¨ FAISS æ„å»ºå‘é‡ç´¢å¼•"""

    def __init__(self, embeddings: np.ndarray):
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç›¸ä¼¼åº¦ï¼ˆå‘é‡å·²å½’ä¸€åŒ–ï¼‰
        self.index.add(embeddings.astype('float32'))
        self.dimension = dimension

    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
        è¿”å›: (ç›¸ä¼¼åº¦åˆ†æ•°, ç´¢å¼•)
        """
        similarities, indices = self.index.search(query_embedding.astype('float32'), top_k)
        return similarities, indices


# ==================== Reranker é‡æ’åº ====================
class BGEReranker:
    """ä½¿ç”¨ BGE-Reranker æ¨¡å‹è¿›è¡Œé‡æ’åº"""

    def __init__(self, model_name: str = Config.RERANKER_MODEL, device: str = Config.RERANKER_DEVICE):
        print(f"æ­£åœ¨åŠ è½½ BGE-Reranker æ¨¡å‹ ({device}æ¨¡å¼)...")
        self.device = device

        # ç¦ç”¨ telemetry
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        import glob
        reranker_cache_pattern = os.path.expanduser(f"~/.cache/huggingface/hub/models--BAAI--bge-reranker-base/snapshots/*")
        reranker_cache_dirs = glob.glob(reranker_cache_pattern)

        use_local = False
        model_to_load = model_name

        if reranker_cache_dirs:
            for snapshot_dir in reranker_cache_dirs:
                if os.path.exists(os.path.join(snapshot_dir, "config.json")):
                    print(f"  ä½¿ç”¨æœ¬åœ° Reranker ç¼“å­˜: {snapshot_dir}")
                    model_to_load = snapshot_dir
                    use_local = True
                    break
        else:
            print(f"  é¦–æ¬¡ä½¿ç”¨ Rerankerï¼Œå°†ä¸‹è½½æ¨¡å‹ï¼ˆçº¦1.1GBï¼‰")

        load_kwargs = {
            "local_files_only": use_local,
            "trust_remote_code": True
        }

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_to_load, **load_kwargs)
            # è®¾ç½® pad tokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_to_load,
                **load_kwargs
            )
            self.model.to(device)
            self.model.eval()
            print("Reranker æ¨¡å‹åŠ è½½å®Œæˆ")

        except Exception as e:
            print(f"\nè­¦å‘Š: Reranker æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°†è·³è¿‡ Reranker æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢ç»“æœ")
            self.model = None
            self.tokenizer = None

    def rerank(self, query: str, candidates: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:
        """å¯¹å€™é€‰ç»“æœè¿›è¡Œé‡æ’åº
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'text': str, 'index': int, 'score': float, ...}
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
        Returns:
            é‡æ’åºåçš„å€™é€‰åˆ—è¡¨
        """
        if self.model is None or not candidates:
            return candidates[:top_k] if top_k else candidates

        if top_k is None:
            top_k = Config.RERANKER_TOP_K

        # å‡†å¤‡è¾“å…¥
        texts = [c.get('content', c.get('text', '')) for c in candidates]

        # é€ä¸ªè®¡ç®—åˆ†æ•°ï¼ˆé¿å… batch size > 1 çš„ pad_token é—®é¢˜ï¼‰
        rerank_scores = []

        for doc_text in texts:
            # Tokenize å•ä¸ªæ ·æœ¬
            inputs = self.tokenizer(
                [[query, doc_text]],  # ä¿æŒ list of lists æ ¼å¼
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(self.device)

            # è®¡ç®—åˆ†æ•°
            with torch.no_grad():
                outputs = self.model(**inputs)
                # å‡è®¾æ¨¡å‹è¾“å‡ºæ˜¯ logitsï¼Œå–ç¬¬ä¸€ä¸ªä½œä¸ºç›¸å…³æ€§åˆ†æ•°
                score = outputs.logits[0][0].item() if outputs.logits.dim() > 1 else outputs.logits.item()
                rerank_scores.append(score)

        # æ›´æ–°åˆ†æ•°å¹¶æ’åº
        for i, candidate in enumerate(candidates):
            candidate['rerank_score'] = float(rerank_scores[i])

        # æŒ‰ rerank_score é™åºæ’åº
        reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)

        return reranked[:top_k]

    def is_available(self) -> bool:
        """æ£€æŸ¥ Reranker æ˜¯å¦å¯ç”¨"""
        return self.model is not None


# ==================== LLM ç²¾åˆ¤ ====================
class LLMJudge:
    """ä½¿ç”¨ LLM åˆ¤æ–­ä¸¤ä¸ªæ®µè½çš„ç›¸å…³æ€§"""

    # ç›¸å…³æ€§ç­‰çº§
    RELEVANCE_NOT_RELATED = "ä¸ç›¸å…³"
    RELEVANCE_WEAK = "å¼±ç›¸å…³"
    RELEVANCE_STRONG = "å¼ºç›¸å…³"

    def __init__(self):
        self.client = openai.OpenAI(
            api_key=Config.LLM_API_KEY,
            base_url=Config.LLM_API_BASE
        )

    def judge(self, text1: str, text2: str) -> Tuple[str, str]:
        """åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬çš„ç›¸å…³æ€§
        è¿”å›: (ç›¸å…³æ€§ç­‰çº§, ç†ç”±è¯´æ˜)
        """
        prompt = self._build_prompt(text1, text2)

        for attempt in range(Config.LLM_MAX_RETRIES):
            try:
                response = self.client.chat.completions.create(
                    model=Config.LLM_MODEL,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´£ä»»æ ‡å‡†æ–‡æ¡£åˆ†æä¸“å®¶ã€‚ä½ éœ€è¦åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬åœ¨'è´£ä»»ä¹‰åŠ¡å±‚é¢'æ˜¯å¦ç›¸å…³ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.1,
                    timeout=Config.LLM_TIMEOUT
                )

                result = response.choices[0].message.content.strip()
                return self._parse_result(result)

            except Exception as e:
                if attempt < Config.LLM_MAX_RETRIES - 1:
                    wait_time = (attempt + 1) * 2
                    print(f"LLM è°ƒç”¨å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•... é”™è¯¯: {e}")
                    time.sleep(wait_time)
                else:
                    print(f"LLM è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    return self.RELEVANCE_NOT_RELATED, f"è°ƒç”¨å¤±è´¥: {str(e)}"

    def _build_prompt(self, text1: str, text2: str) -> str:
        """æ„å»º LLM åˆ¤æ–­æç¤ºè¯"""
        return f"""è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤æ®µè´£ä»»æ ‡å‡†æ¡æ¬¾åœ¨"è´£ä»»ä¹‰åŠ¡å±‚é¢"æ˜¯å¦ç›¸å…³ã€‚

ã€æ¡æ¬¾ Aã€‘ï¼š
{text1}

ã€æ¡æ¬¾ Bã€‘ï¼š
{text2}

ä»ä»¥ä¸‹ç»´åº¦åˆ¤æ–­ï¼š
1. æ˜¯å¦æ¶‰åŠç›¸ä¼¼çš„è´£ä»»æˆ–ä¹‰åŠ¡ä¸»é¢˜
2. æ˜¯å¦è§„å®šç›¸ä¼¼çš„è¦æ±‚æˆ–æ ‡å‡†
3. è¦†ç›–èŒƒå›´å…³ç³»ï¼ˆå®Œå…¨ä¸€è‡´/éƒ¨åˆ†è¦†ç›–/äº’è¡¥ï¼‰
4. ä¸¥æ ¼ç¨‹åº¦å·®å¼‚


è¯·ä»…è¿”å›ä»¥ä¸‹æ ¼å¼çš„ç»“æœï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š
ç›¸å…³æ€§ï¼š[ä¸ç›¸å…³/å¼±ç›¸å…³/å¼ºç›¸å…³]
ç†ç”±ï¼š["åŒ¹é…ç±»å‹": "ä¸¤æ¡æ¬¾å®Œå…¨ä¸€è‡´", "ä¸¤æ¡æ¬¾éƒ¨åˆ†è¦†ç›–", "ä¸¤æ¡æ¬¾è¡¥å……è¯´æ˜";
        ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±ï¼Œä¸è¶…è¿‡100å­—]
"""

    def _parse_result(self, result: str) -> Tuple[str, str]:
        """è§£æ LLM è¿”å›ç»“æœ"""
        result = result.strip()

        # æå–ç›¸å…³æ€§ç­‰çº§
        relevance = self.RELEVANCE_NOT_RELATED
        if self.RELEVANCE_STRONG in result:
            relevance = self.RELEVANCE_STRONG
        elif self.RELEVANCE_WEAK in result:
            relevance = self.RELEVANCE_WEAK
        elif self.RELEVANCE_NOT_RELATED in result:
            relevance = self.RELEVANCE_NOT_RELATED

        # æå–ç†ç”±
        reason = ""
        if "ç†ç”±ï¼š" in result:
            reason = result.split("ç†ç”±ï¼š", 1)[1].strip()
        elif "Reason:" in result:
            reason = result.split("Reason:", 1)[1].strip()

        return relevance, reason


# ==================== ä¸»åŒ¹é…æµç¨‹ ====================
class TextMatcher:
    """æ–‡æœ¬åŒ¹é…ä¸»æµç¨‹"""

    def __init__(self):
        print("=" * 60)
        print("é€šç”¨æ¡æ¬¾åŒ¹é…ç³»ç»Ÿ")
        print("=" * 60)

        # åŠ è½½æ–‡æ¡£
        print("\n[1/4] åŠ è½½æ–‡æ¡£...")
        all_a_docs = load_json_documents(Config.A_FILE)
        all_b_docs = load_json_documents(Config.B_FILE)

        # åªä½¿ç”¨ content å­—æ®µï¼Œè¿‡æ»¤ Preamble
        self.a_docs = filter_content_blocks(all_a_docs)
        self.b_docs = filter_content_blocks(all_b_docs)

        print(f"  - Aæ–‡ä»¶: {len(all_a_docs)} æ¡ï¼ˆå…¶ä¸­ content: {len(self.a_docs)} æ¡ï¼‰")
        print(f"  - Bæ–‡ä»¶: {len(all_b_docs)} æ¡ï¼ˆå…¶ä¸­ content: {len(self.b_docs)} æ¡ï¼‰")

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print("\n[2/4] åˆå§‹åŒ– Qwen3-Embedding-8B åµŒå…¥æ¨¡å‹...")
        self.embedder = QwenEmbedder()

        # æ„å»ºBæ–‡ä»¶æ–‡æ¡£å‘é‡ç´¢å¼•
        print("\n[3/4] æ„å»ºBæ–‡ä»¶æ–‡æ¡£å‘é‡ç´¢å¼•...")
        # ä½¿ç”¨å¸¦å±‚çº§è·¯å¾„çš„æ–‡æœ¬è¿›è¡Œ embedding
        b_texts = [build_embedding_text(doc) for doc in self.b_docs]
        # B æ–‡æ¡£ä½œä¸º passageï¼ˆæ–‡æ¡£åº“ï¼‰
        b_embeddings = self.embedder.encode_passages(b_texts)
        self.vector_index = VectorIndex(b_embeddings)
        print(f"  - å‘é‡ç»´åº¦: {b_embeddings.shape[1]}")
        print(f"  - ç´¢å¼•å®Œæˆ")

        # åˆå§‹åŒ– LLM åˆ¤æ–­å™¨
        print("\n[4/5] åˆå§‹åŒ– LLM åˆ¤æ–­å™¨...")
        self.llm_judge = LLMJudge()
        print("  - API åœ°å€:", Config.LLM_API_BASE)
        print("  - æ¨¡å‹:", Config.LLM_MODEL)

        # åˆå§‹åŒ– Reranker
        print("\n[5/5] åˆå§‹åŒ– Reranker...")
        if Config.ENABLE_RERANKER:
            self.reranker = BGEReranker()
            if self.reranker.is_available():
                print(f"  - Reranker å·²å¯ç”¨ (Top-K: {Config.RERANKER_TOP_K})")
            else:
                print("  - Reranker åŠ è½½å¤±è´¥ï¼Œå°†è·³è¿‡é‡æ’åºæ­¥éª¤")
        else:
            self.reranker = None
            print("  - Reranker å·²ç¦ç”¨ï¼ˆENABLE_RERANKER = Falseï¼‰")

        print("\n" + "=" * 60)
        print("åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹åŒ¹é…...")
        print("=" * 60 + "\n")

        # ä¿å­˜æ–‡æ¡£æ•°é‡ç»Ÿè®¡å’Œæ–‡ä»¶åï¼ˆå»æ‰ .json åç¼€ï¼‰
        self.doc_counts = {
            'a_docs': len(self.a_docs),
            'b_docs': len(self.b_docs),
            'a_file_name': os.path.basename(Config.A_FILE).replace('.json', ''),
            'b_file_name': os.path.basename(Config.B_FILE).replace('.json', '')
        }

    def match(self) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒåŒ¹é…æµç¨‹ï¼šå‘é‡æ£€ç´¢ -> Rerank -> LLM ç²¾åˆ¤"""
        results = []

        # å¯¹Aæ–‡ä»¶çš„æ¯ä¸ªcontentæ®µè½è¿›è¡ŒåŒ¹é…
        for a_doc in tqdm(self.a_docs, desc="åŒ¹é…è¿›åº¦"):
            # ä½¿ç”¨å¸¦å±‚çº§è·¯å¾„çš„æ–‡æœ¬è¿›è¡Œå‘é‡æ£€ç´¢
            a_text_for_embedding = build_embedding_text(a_doc)
            a_text = a_doc.get('content', '')  # ç”¨äºå±•ç¤ºçš„åŸå§‹å†…å®¹

            # 1. å‘é‡æ£€ç´¢ Top-Kï¼ˆå¬å›æ›´å¤šå€™é€‰ï¼‰
            # ä½¿ç”¨å¸¦å±‚çº§è·¯å¾„çš„æ–‡æœ¬è¿›è¡ŒæŸ¥è¯¢
            # A æ–‡æ¡£ä½œä¸º queryï¼ˆæŸ¥è¯¢ï¼‰
            query_embedding = self.embedder.encode_queries([a_text_for_embedding])
            similarities, indices = self.vector_index.search(query_embedding, Config.TOP_K)

            # 2. å‡†å¤‡å€™é€‰åˆ—è¡¨
            candidates = []
            for similarity, idx in zip(similarities[0], indices[0]):
                if similarity < Config.SIMILARITY_THRESHOLD * 0.5:  # é™ä½é˜ˆå€¼ï¼Œè®© Reranker æ¥ç­›é€‰
                    continue
                b_doc = self.b_docs[idx]
                candidates.append({
                    'doc': b_doc,
                    'content': b_doc.get('content', ''),
                    'similarity': float(similarity),
                    'index': idx
                })

            if not candidates:
                # æ²¡æœ‰å€™é€‰ï¼Œæ·»åŠ ç©ºåŒ¹é…
                results.append({
                    'Aæ–‡ä»¶æ¡æ¬¾': a_text,
                    'Bæ–‡ä»¶æ¡æ¬¾': '',
                    'å‘é‡ç›¸ä¼¼åº¦': '',
                    'Rerankåˆ†æ•°': '',
                    'æ’å': '',
                    'LLMåˆ¤æ–­ç»“æœ': '',
                    'LLMåˆ¤æ–­ç†ç”±': '',
                    'Bæ–‡ä»¶è·¯å¾„': '',
                    'Aæ–‡ä»¶è·¯å¾„': a_doc.get('path', ''),
                })
                continue

            # 3. Rerank é‡æ’åº
            if self.reranker is not None and self.reranker.is_available():
                # ä½¿ç”¨å¸¦å±‚çº§è·¯å¾„çš„æ–‡æœ¬è¿›è¡Œ rerank
                reranked = self.reranker.rerank(a_text_for_embedding, candidates, top_k=Config.RERANKER_TOP_K)
                top_candidates = reranked
            else:
                # Reranker ä¸å¯ç”¨æˆ–å·²ç¦ç”¨ï¼Œç›´æ¥ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ’åº
                top_candidates = sorted(candidates, key=lambda x: x['similarity'], reverse=True)[:Config.RERANKER_TOP_K]

            # 4. å¯¹ Top-K ç»“æœè¿›è¡Œ LLM ç²¾åˆ¤
            has_match = False
            for rank, candidate in enumerate(top_candidates, 1):
                b_doc = candidate['doc']
                # è·å–Bæ–‡æ¡£ç”¨äºå±•ç¤ºçš„åŸå§‹å†…å®¹
                b_text = b_doc.get('content', '')

                # LLM ç²¾åˆ¤ï¼ˆä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œåˆ¤æ–­ï¼Œä¸åŒ…å«å±‚çº§è·¯å¾„ï¼‰
                llm_relevance, llm_reason = self.llm_judge.judge(a_text, b_text)

                # ä¿å­˜ç»“æœ
                result = {
                    'Aæ–‡ä»¶æ¡æ¬¾': a_text,
                    'Bæ–‡ä»¶æ¡æ¬¾': b_text,
                    'å‘é‡ç›¸ä¼¼åº¦': round(candidate['similarity'], 4),
                    'Rerankåˆ†æ•°': round(candidate.get('rerank_score', 0), 4),
                    'æ’å': rank,
                    'LLMåˆ¤æ–­ç»“æœ': llm_relevance,
                    'LLMåˆ¤æ–­ç†ç”±': llm_reason,
                    'Bæ–‡ä»¶è·¯å¾„': b_doc.get('path', ''),
                    'Aæ–‡ä»¶è·¯å¾„': a_doc.get('path', ''),
                }
                results.append(result)
                has_match = True

            # å¦‚æœæ²¡æœ‰åŒ¹é…ç»“æœï¼Œæ·»åŠ ç©ºåŒ¹é…
            if not has_match:
                results.append({
                    'Aæ–‡ä»¶æ¡æ¬¾': a_text,
                    'Bæ–‡ä»¶æ¡æ¬¾': '',
                    'å‘é‡ç›¸ä¼¼åº¦': '',
                    'Rerankåˆ†æ•°': '',
                    'æ’å': '',
                    'LLMåˆ¤æ–­ç»“æœ': '',
                    'LLMåˆ¤æ–­ç†ç”±': '',
                    'Bæ–‡ä»¶è·¯å¾„': '',
                    'Aæ–‡ä»¶è·¯å¾„': a_doc.get('path', ''),
                })

        return results

    def export_to_excel(self, results: List[Dict[str, Any]], output_path: str = None):
        """å¯¼å‡ºç»“æœåˆ° Excelï¼Œç›¸åŒçš„"Aæ–‡ä»¶æ¡æ¬¾"åˆå¹¶å•å…ƒæ ¼"""
        if output_path is None:
            output_path = Config.OUTPUT_EXCEL

        print(f"\næ­£åœ¨å¯¼å‡ºç»“æœåˆ° {output_path}...")

        df = pd.DataFrame(results)

        # è°ƒæ•´åˆ—é¡ºåº
        columns_order = [
            'Aæ–‡ä»¶æ¡æ¬¾',
            'Bæ–‡ä»¶æ¡æ¬¾',
            'å‘é‡ç›¸ä¼¼åº¦',
            'Rerankåˆ†æ•°',
            'LLMåˆ¤æ–­ç»“æœ',
            'LLMåˆ¤æ–­ç†ç”±',
            'æ’å',
            'Aæ–‡ä»¶è·¯å¾„',
            'Bæ–‡ä»¶è·¯å¾„',
        ]

        # åªä¿ç•™å­˜åœ¨çš„åˆ—
        columns_order = [col for col in columns_order if col in df.columns]
        df = df[columns_order]

        # å¯¼å‡ºåˆ° Excel
        from openpyxl.styles import Alignment, Font, Border, Side
        from openpyxl.utils import get_column_letter

        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='åŒ¹é…ç»“æœ')
            worksheet = writer.sheets['åŒ¹é…ç»“æœ']

            # è°ƒæ•´åˆ—å®½
            worksheet.column_dimensions['A'].width = 60  # Aæ–‡ä»¶æ¡æ¬¾
            worksheet.column_dimensions['B'].width = 60  # Bæ–‡ä»¶æ¡æ¬¾
            worksheet.column_dimensions['C'].width = 15  # å‘é‡ç›¸ä¼¼åº¦
            worksheet.column_dimensions['D'].width = 15  # Rerankåˆ†æ•°
            worksheet.column_dimensions['E'].width = 15  # LLMåˆ¤æ–­ç»“æœ
            worksheet.column_dimensions['F'].width = 40  # LLMåˆ¤æ–­ç†ç”±
            worksheet.column_dimensions['G'].width = 10  # æ’å
            worksheet.column_dimensions['H'].width = 40  # Aæ–‡ä»¶è·¯å¾„
            worksheet.column_dimensions['I'].width = 40  # Bæ–‡ä»¶è·¯å¾„

            # è®¾ç½®æ‰€æœ‰æ•°æ®è¡Œçš„è¡Œé«˜ä¸º 200
            for row in range(2, len(df) + 2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯æ ‡é¢˜ï¼‰
                worksheet.row_dimensions[row].height = 200

            # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼
            header_font = Font(bold=True, size=11)
            header_alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
            thin_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )

            for cell in worksheet[1]:
                cell.font = header_font
                cell.alignment = header_alignment
                cell.border = thin_border

            # åˆå¹¶ç›¸åŒçš„Aæ–‡ä»¶æ¡æ¬¾å•å…ƒæ ¼
            # ä»æ•°æ®è¡Œå¼€å§‹ï¼ˆç¬¬2è¡Œï¼ŒExcelç´¢å¼•ä¸º2ï¼‰
            start_row = 2
            current_value = None
            merge_start_row = 2

            for row_idx in range(2, len(df) + 2):
                cell_value = worksheet.cell(row=row_idx, column=1).value

                if cell_value != current_value:
                    # å¦‚æœä¹‹å‰çš„å€¼ç›¸åŒä¸”æœ‰å¤šè¡Œï¼Œåˆå¹¶å•å…ƒæ ¼
                    if current_value is not None and merge_start_row < row_idx - 1:
                        worksheet.merge_cells(f'A{merge_start_row}:A{row_idx - 1}')
                        # è®¾ç½®åˆå¹¶åçš„å•å…ƒæ ¼å¯¹é½æ–¹å¼
                        merged_cell = worksheet.cell(row=merge_start_row, column=1)
                        merged_cell.alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)

                    current_value = cell_value
                    merge_start_row = row_idx

            # å¤„ç†æœ€åä¸€ç»„ç›¸åŒçš„å€¼
            if merge_start_row < len(df) + 2:
                worksheet.merge_cells(f'A{merge_start_row}:A{len(df) + 1}')
                merged_cell = worksheet.cell(row=merge_start_row, column=1)
                merged_cell.alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)

            # è®¾ç½®æ‰€æœ‰æ•°æ®è¡Œçš„å¯¹é½æ–¹å¼å’Œè¾¹æ¡†
            for row in range(2, len(df) + 2):
                for col in range(1, len(columns_order) + 1):
                    cell = worksheet.cell(row=row, column=col)
                    cell.alignment = Alignment(
                        horizontal='left' if col in [1, 2, 5, 7, 8] else 'center',
                        vertical='top',
                        wrap_text=True
                    )
                    cell.border = thin_border

        print(f"å¯¼å‡ºå®Œæˆï¼å…± {len(results)} æ¡è®°å½•")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = self._calculate_stats(results)
        return stats

    def _calculate_stats(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»“æœç»Ÿè®¡ä¿¡æ¯"""
        relevance_counts = {}
        empty_match = 0
        for r in results:
            if r['LLMåˆ¤æ–­ç»“æœ'] == '':
                empty_match += 1
            else:
                relevance = r['LLMåˆ¤æ–­ç»“æœ']
                relevance_counts[relevance] = relevance_counts.get(relevance, 0) + 1

        return {
            'total': len(results),
            'empty_match': empty_match,
            'relevance_counts': relevance_counts
        }

    def export_to_html(self, results: List[Dict[str, Any]], stats: Dict[str, Any], doc_counts: Dict[str, int], output_path: str = None):
        """å¯¼å‡ºç»“æœåˆ° HTMLï¼Œå¸¦ç¾è§‚æ ·å¼å’Œç»Ÿè®¡ä¿¡æ¯"""
        if output_path is None:
            output_path = Config.OUTPUT_HTML

        print(f"\næ­£åœ¨å¯¼å‡ºç»“æœåˆ° {output_path}...")

        df = pd.DataFrame(results)

        # è°ƒæ•´åˆ—é¡ºåº
        columns_order = [
            'Aæ–‡ä»¶æ¡æ¬¾',
            'Bæ–‡ä»¶æ¡æ¬¾',
            'å‘é‡ç›¸ä¼¼åº¦',
            'Rerankåˆ†æ•°',
            'LLMåˆ¤æ–­ç»“æœ',
            'LLMåˆ¤æ–­ç†ç”±',
            'æ’å',
            'Aæ–‡ä»¶è·¯å¾„',
            'Bæ–‡ä»¶è·¯å¾„',
        ]
        columns_order = [col for col in columns_order if col in df.columns]
        df = df[columns_order]

        # ç”Ÿæˆ HTML
        html_content = self._generate_html(df, columns_order, stats, doc_counts)

        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"HTML å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶è·¯å¾„: {output_path}")

    def _generate_html(self, df: pd.DataFrame, columns_order: List[str], stats: Dict[str, Any], doc_counts: Dict[str, int]) -> str:
        """ç”Ÿæˆå®Œæ•´çš„ HTML å†…å®¹"""

        # è®¡ç®—åˆå¹¶å•å…ƒæ ¼çš„ rowspan
        merge_spans = self._calculate_merge_spans(df)

        # ç»Ÿè®¡ä¿¡æ¯ HTML
        stats_html = self._generate_stats_html(stats, doc_counts)

        # è¡¨æ ¼å†…å®¹ HTML
        table_html = self._generate_table_html(df, columns_order, merge_spans, doc_counts)

        # ç»„è£…å®Œæ•´ HTML
        html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RBA Clause Matching Results</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}

        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }}

        .container {{
            max-width: 1800px;
            margin: 0 auto;
            background: white;
            border-radius: 16px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }}

        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}

        .header h1 {{
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }}

        .header .subtitle {{
            font-size: 1.1rem;
            opacity: 0.9;
        }}

        .stats-section {{
            padding: 30px 40px;
            background: #f8f9fa;
            border-bottom: 2px solid #e9ecef;
        }}

        .stats-title {{
            font-size: 1.3rem;
            font-weight: 600;
            color: #495057;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
        }}

        .stats-title::before {{
            content: "ğŸ“Š";
            margin-right: 10px;
            font-size: 1.5rem;
        }}

        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
        }}

        .stat-card {{
            background: white;
            border-radius: 12px;
            padding: 20px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border-left: 4px solid #667eea;
            transition: transform 0.2s, box-shadow 0.2s;
        }}

        .stat-card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.12);
        }}

        .stat-card.strong {{
            border-left-color: #28a745;
        }}

        .stat-card.weak {{
            border-left-color: #ffc107;
        }}

        .stat-card.not-related {{
            border-left-color: #dc3545;
        }}

        .stat-card.empty {{
            border-left-color: #e3e0e0;
        }}

        .stat-label {{
            font-size: 0.9rem;
            color: #6c757d;
            margin-bottom: 8px;
        }}

        .stat-value {{
            font-size: 2rem;
            font-weight: 700;
            color: #212529;
        }}

        .table-section {{
            padding: 40px;
            overflow-x: auto;
        }}

        table {{
            width: 100%;
            border-collapse: collapse;
            font-size: 0.95rem;
        }}

        thead {{
            position: sticky;
            top: 0;
            z-index: 10;
        }}

        th {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-weight: 600;
            padding: 16px 12px;
            text-align: center;
            position: relative;
            white-space: nowrap;
        }}

        th:first-child {{
            border-top-left-radius: 8px;
        }}

        th:last-child {{
            border-top-right-radius: 8px;
        }}

        td {{
            padding: 15px 12px;
            border-bottom: 1px solid #dee2e6;
            border-right: 1px solid #dee2e6;
            vertical-align: top;
            background: white;
        }}

        td:last-child {{
            border-right: none;
        }}

        tbody tr:hover {{
            background: #f8f9fa;
        }}

        /* æ¡æ¬¾å†…å®¹åˆ— - åŠ å¤§å®½åº¦æ¯”ä¾‹ */
        td.clause-a,
        td.clause-b {{
            text-align: left;
            vertical-align: top;
            font-size: 0.95rem;
            line-height: 1.8;
            max-width: 800px;
            min-width: 300px;
            white-space: pre-wrap;
            word-wrap: break-word;
        }}

        /* è·¯å¾„åˆ— - ç¼©å°å®½åº¦æ¯”ä¾‹ */
        td.path {{
            text-align: left;
            font-size: 0.85rem;
            color: #6c757d;
            font-family: "Courier New", monospace;
            max-width: 250px;
            min-width: 100px;
        }}

        /* æ•°å€¼åˆ— */
        td.score,
        td.rank {{
            text-align: center;
            font-weight: 500;
        }}

        /* ç›¸å…³æ€§æ ‡ç­¾ */
        .relevance-badge {{
            display: inline-block;
            padding: 6px 16px;
            border-radius: 20px;
            font-weight: 600;
            font-size: 0.9rem;
            text-align: center;
        }}

        .relevance-strong {{
            background: #d4edda;
            color: #155724;
        }}

        .relevance-weak {{
            background: #fff3cd;
            color: #856404;
        }}

        .relevance-not-related {{
            background: #f8d7da;
            color: #721c24;
        }}

        /* ç†ç”±åˆ— */
        td.reason {{
            text-align: left;
            font-size: 0.9rem;
            color: #495057;
            line-height: 1.6;
            max-width: 400px;
        }}

        /* ç©ºåŒ¹é…è¡Œ */
        tr.empty-match {{
            background: #fff5f5 !important;
        }}

        tr.empty-match td {{
            color: #999;
            font-style: italic;
        }}

        /* åˆ†éš”çº¿ */
        .divider {{
            height: 1px;
            background: linear-gradient(90deg, transparent, #dee2e6, transparent);
            margin: 20px 0;
        }}

        /* å“åº”å¼ */
        @media (max-width: 768px) {{
            .stats-grid {{
                grid-template-columns: 1fr;
            }}

            table {{
                font-size: 0.85rem;
            }}

            th, td {{
                padding: 10px 8px;
            }}
        }}

        /* æ‰“å°æ ·å¼ */
        @media print {{
            body {{
                background: white;
                padding: 0;
            }}

            .container {{
                box-shadow: none;
                border-radius: 0;
            }}

            .header {{
                background: #333 !important;
                -webkit-print-color-adjust: exact;
                print-color-adjust: exact;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        {self._generate_header_html(df)}
        {stats_html}
        {table_html}
    </div>
</body>
</html>"""
        return html

    def _generate_header_html(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆé¡µé¢å¤´éƒ¨ HTML"""
        return f"""        <div class="header">
            <h1>RBA Clause Matching Results</h1>
            <div class="subtitle">
                Generation Time: {time.strftime('%Y-%m-%d %H:%M:%S')}
            </div>
        </div>"""

    def _generate_stats_html(self, stats: Dict[str, Any], doc_counts: Dict[str, int]) -> str:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯ HTML"""
        relevance_counts = stats['relevance_counts']

        cards = []

        # A/B æ–‡æ¡£æ•°é‡å¡ç‰‡ï¼ˆä½¿ç”¨ JSON æ–‡ä»¶åï¼‰
        cards.append(f"""                <div class="stat-card">
                    <div class="stat-label">ğŸ“„ {doc_counts.get('a_file_name', 'A æ–‡æ¡£')}</div>
                    <div class="stat-value">{doc_counts.get('a_docs', 0)}</div>
                </div>""")
        cards.append(f"""                <div class="stat-card">
                    <div class="stat-label">ğŸ“„ {doc_counts.get('b_file_name', 'B æ–‡æ¡£')}</div>
                    <div class="stat-value">{doc_counts.get('b_docs', 0)}</div>
                </div>""")

        # æ€»æ•°å¡ç‰‡
        cards.append(f"""                <div class="stat-card">
                    <div class="stat-label">æ€»è®°å½•æ•°</div>
                    <div class="stat-value">{stats['total']}</div>
                </div>""")

        # ç©ºåŒ¹é…å¡ç‰‡
        if stats['empty_match'] > 0:
            cards.append(f"""                <div class="stat-card empty">
                    <div class="stat-label">ç©ºåŒ¹é…ï¼ˆæ— åŒ¹é…ç»“æœï¼‰</div>
                    <div class="stat-value">{stats['empty_match']}</div>
                </div>""")

        # ç›¸å…³æ€§ç»Ÿè®¡å¡ç‰‡ï¼ˆæŒ‰æŒ‡å®šé¡ºåºï¼šå¼ºç›¸å…³ > å¼±ç›¸å…³ > ä¸ç›¸å…³ï¼‰
        relevance_order = ['å¼ºç›¸å…³', 'å¼±ç›¸å…³', 'ä¸ç›¸å…³']
        for relevance in relevance_order:
            if relevance in relevance_counts:
                count = relevance_counts[relevance]
                css_class = 'strong' if relevance == 'å¼ºç›¸å…³' else ('weak' if relevance == 'å¼±ç›¸å…³' else 'not-related')
                icon = 'ğŸŸ¢' if relevance == 'å¼ºç›¸å…³' else ('ğŸŸ¡' if relevance == 'å¼±ç›¸å…³' else 'ğŸ”´')
                cards.append(f"""                <div class="stat-card {css_class}">
                    <div class="stat-label">{icon} {relevance}</div>
                    <div class="stat-value">{count}</div>
                </div>""")

        return f"""        <div class="stats-section">
            <div class="stats-title">åŒ¹é…ç»Ÿè®¡</div>
            <div class="stats-grid">
{chr(10).join(cards)}
            </div>
        </div>"""

    def _calculate_merge_spans(self, df: pd.DataFrame) -> dict:
        """è®¡ç®—éœ€è¦åˆå¹¶çš„å•å…ƒæ ¼çš„ rowspan"""
        merge_spans = {}
        start_row = 0
        current_value = None
        merge_start_row = 0

        for row_idx in range(len(df)):
            cell_value = df.iloc[row_idx]['Aæ–‡ä»¶æ¡æ¬¾']

            if cell_value != current_value:
                if current_value is not None and merge_start_row < row_idx:
                    span = row_idx - merge_start_row
                    for r in range(merge_start_row, row_idx):
                        merge_spans[r] = span if r == merge_start_row else 0

                current_value = cell_value
                merge_start_row = row_idx

        # å¤„ç†æœ€åä¸€ç»„
        if merge_start_row < len(df):
            span = len(df) - merge_start_row
            for r in range(merge_start_row, len(df)):
                merge_spans[r] = span if r == merge_start_row else 0

        return merge_spans

    def _generate_table_html(self, df: pd.DataFrame, columns_order: List[str], merge_spans: dict, doc_counts: Dict[str, int]) -> str:
        """ç”Ÿæˆè¡¨æ ¼ HTML"""

        # è¡¨å¤´ï¼ˆä½¿ç”¨æ–‡ä»¶åï¼Œå·²å»æ‰ .json åç¼€ï¼‰
        a_name = doc_counts.get('a_file_name', 'A æ–‡æ¡£')
        b_name = doc_counts.get('b_file_name', 'B æ–‡æ¡£')

        header_mapping = {
            'Aæ–‡ä»¶æ¡æ¬¾': f'{a_name}',
            'Bæ–‡ä»¶æ¡æ¬¾': f'{b_name}',
            'å‘é‡ç›¸ä¼¼åº¦': 'Vector_Score',
            'Rerankåˆ†æ•°': 'Rerank_Score',
            'LLMåˆ¤æ–­ç»“æœ': 'Relevance_label',
            'LLMåˆ¤æ–­ç†ç”±': 'LLM_Rationale',
            'æ’å': 'Rank',
            'Aæ–‡ä»¶è·¯å¾„': f'{a_name} clause path',
            'Bæ–‡ä»¶è·¯å¾„': f'{b_name} clause path',
        }

        headers = [header_mapping.get(col, col) for col in columns_order]

        thead_html = "        <thead>\n            <tr>\n"
        for h in headers:
            thead_html += f"                <th>{h}</th>\n"
        thead_html += "            </tr>\n        </thead>"

        # è¡¨ä½“
        tbody_html = "        <tbody>\n"

        for row_idx, row in df.iterrows():
            # åˆ¤æ–­æ˜¯å¦ä¸ºç©ºåŒ¹é…è¡Œ
            is_empty = row['LLMåˆ¤æ–­ç»“æœ'] == ''
            tr_class = ' class="empty-match"' if is_empty else ''

            tbody_html += f"            <tr{tr_class}>\n"

            for col_idx, col in enumerate(columns_order):
                value = row[col]

                # å¤„ç† A æ–‡ä»¶æ¡æ¬¾çš„åˆå¹¶å•å…ƒæ ¼
                if col == 'Aæ–‡ä»¶æ¡æ¬¾':
                    rowspan = merge_spans.get(row_idx, 1)
                    if rowspan == 0:
                        continue  # è·³è¿‡è¢«åˆå¹¶çš„å•å…ƒæ ¼
                    rowspan_attr = f' rowspan="{rowspan}"' if rowspan > 1 else ''
                else:
                    rowspan_attr = ''

                # ç¡®å®šå•å…ƒæ ¼çš„ CSS ç±»
                cell_class_attr = self._get_cell_class_attr(col, value, is_empty)

                # æ ¼å¼åŒ–å•å…ƒæ ¼å†…å®¹
                cell_content = self._format_cell_content(col, value, is_empty)

                # æ‹¼æ¥å•å…ƒæ ¼ HTML (ç¡®ä¿ class å‰æœ‰ç©ºæ ¼)
                class_space = ' ' if cell_class_attr else ''
                tbody_html += f'                <td{class_space}{cell_class_attr}{rowspan_attr}>{cell_content}</td>\n'

            tbody_html += "            </tr>\n"

        tbody_html += "        </tbody>"

        return f"""        <div class="table-section">
            <table>
{thead_html}
{tbody_html}
            </table>
        </div>"""

    def _get_cell_class_attr(self, col: str, value: Any, is_empty: bool) -> str:
        """è·å–å•å…ƒæ ¼çš„ class å±æ€§å­—ç¬¦ä¸²ï¼ˆå« class= å‰ç¼€ï¼‰"""
        if is_empty:
            return ''

        class_map = {
            'Aæ–‡ä»¶æ¡æ¬¾': 'class="clause-a"',
            'Bæ–‡ä»¶æ¡æ¬¾': 'class="clause-b"',
            'Aæ–‡ä»¶è·¯å¾„': 'class="path"',
            'Bæ–‡ä»¶è·¯å¾„': 'class="path"',
            'å‘é‡ç›¸ä¼¼åº¦': 'class="score"',
            'Rerankåˆ†æ•°': 'class="score"',
            'æ’å': 'class="rank"',
            'LLMåˆ¤æ–­ç†ç”±': 'class="reason"',
        }
        return class_map.get(col, '')

    def _format_cell_content(self, col: str, value: Any, is_empty: bool) -> str:
        """æ ¼å¼åŒ–å•å…ƒæ ¼å†…å®¹"""
        if pd.isna(value) or value == '':
            return '<span style="color: #999;">â€”</span>'

        if col == 'LLMåˆ¤æ–­ç»“æœ':
            css_class = ''
            if value == 'å¼ºç›¸å…³':
                css_class = 'relevance-strong'
            elif value == 'å¼±ç›¸å…³':
                css_class = 'relevance-weak'
            elif value == 'ä¸ç›¸å…³':
                css_class = 'relevance-not-related'
            return f'<span class="relevance-badge {css_class}">{value}</span>'

        if col in ['å‘é‡ç›¸ä¼¼åº¦', 'Rerankåˆ†æ•°']:
            return f'{value:.4f}'

        # HTML è½¬ä¹‰
        if isinstance(value, str):
            value = value.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')

        return str(value)


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
def main():
    """ä¸»ç¨‹åº"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(Config.A_FILE):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {Config.A_FILE}")
        return

    if not os.path.exists(Config.B_FILE):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {Config.B_FILE}")
        return

    # åˆ›å»ºåŒ¹é…å™¨å¹¶æ‰§è¡ŒåŒ¹é…
    matcher = TextMatcher()
    results = matcher.match()

    # å¯¼å‡ºç»“æœ
    if results:
        # å¯¼å‡º Excel
        stats = matcher.export_to_excel(results)

        # å¯¼å‡º HTML
        matcher.export_to_html(results, stats, matcher.doc_counts)

        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print("\n" + "=" * 60)
        print("å¯¼å‡ºå®Œæˆï¼")
        print(f"  - Excel: {Config.OUTPUT_EXCEL}")
        print(f"  - HTML:  {Config.OUTPUT_HTML}")
        print("=" * 60)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\nç»“æœç»Ÿè®¡:")
        print(f"  - æ€»è®°å½•æ•°: {stats['total']}")
        print(f"  - ç©ºåŒ¹é…ï¼ˆæ— åŒ¹é…ç»“æœï¼‰: {stats['empty_match']} æ¡")
        for relevance, count in stats['relevance_counts'].items():
            print(f"  - {relevance}: {count} æ¡")

    else:
        print("\næ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœ")


if __name__ == "__main__":
    main()
